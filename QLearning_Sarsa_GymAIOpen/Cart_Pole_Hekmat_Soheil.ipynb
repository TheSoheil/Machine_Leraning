{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This program is developed by Soheil Hekmat for Question2 par c(Cart pole)"
      ],
      "metadata": {
        "id": "Cs4oNZInmh2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "V5MDlEYVmp_0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1yQNU8AMP8xH"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import trange\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "%matplotlib inline\n",
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making environment of Cliff walking using 'gym AI toy text' and specifying it\n",
        "\n",
        "we wanna see environment's features"
      ],
      "metadata": {
        "id": "mOauHG_qms58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "# env = gym.make('FrozenLake-v1', desc=None, map_name=\"8x8\", is_slippery=False)\n",
        "spec = gym.spec('CartPole-v1')\n",
        "\n",
        "print(f\"Action Space: {env.action_space}\")\n",
        "print(f\"Observation Space: {env.observation_space}\")\n",
        "print(f\"Max Episode Steps: {spec.max_episode_steps}\")\n",
        "print(f\"Nondeterministic: {spec.nondeterministic}\")\n",
        "print(f\"Reward Range: {env.reward_range}\")\n",
        "print(f\"Reward Threshold: {spec.reward_threshold}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENPWYayCP-6O",
        "outputId": "f50bf6c0-4ca0-4caa-b681-1aa70774587d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Discrete(2)\n",
            "Observation Space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
            "Max Episode Steps: 500\n",
            "Nondeterministic: False\n",
            "Reward Range: (-inf, inf)\n",
            "Reward Threshold: 475.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Actions Related to Environment"
      ],
      "metadata": {
        "id": "4EtN3bLTmwGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Actions =  {0: 'LEFT',\n",
        "            1: 'RIGHT'}"
      ],
      "metadata": {
        "id": "GZMWVQl4QH_3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "taking some random actions to see how cliff walking works"
      ],
      "metadata": {
        "id": "2PQLrb-mmyLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Be free to test!\n",
        "# take random actions, and see how the agent moves\n",
        "\n",
        "env.reset()\n",
        "\n",
        "action_seq= []\n",
        "\n",
        "for i in range(1):\n",
        "  action_number = env.action_space.sample()\n",
        "  action_seq.append(Actions[action_number])\n",
        "  info = env.step(action_number)\n",
        "  print(env.step(action_number))   # Output: (int(s), r, termination, False, {\"prob\": p})\n",
        "    # here we Print the results after action\n",
        "  print(\"New Observation:\", list(info)[0])\n",
        "  print(\"Reward:\", list(info)[1])\n",
        "  print(\"Episode Done:\", list(info)[2])\n",
        "\n",
        "print(' --> '.join(action_seq))\n",
        "env_screen = env.render(mode = 'rgb_array',)\n",
        "plt.imshow(env_screen)\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "1BowcQ62QK5d",
        "outputId": "86be300d-0b53-433b-feaa-1fa50013e192"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([-0.01331077, -0.37964198,  0.03820978,  0.62852186], dtype=float32), 1.0, False, {})\n",
            "New Observation: [-0.0096291  -0.18408352  0.03168946  0.3260163 ]\n",
            "Reward: 1.0\n",
            "Episode Done: False\n",
            "LEFT\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QLearning"
      ],
      "metadata": {
        "id": "efkE3JNjm07U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defininig and Tuning Hyperparameters"
      ],
      "metadata": {
        "id": "iyoqUYE4m34N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10_000  # Total training episodes\n",
        "learning_rate = 0.01         # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 1_00        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability\n",
        "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "0yasO4CXQOPd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descretizing the continious env"
      ],
      "metadata": {
        "id": "mcd1saQxnd3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discretize_state(state, bins):\n",
        "    # Discretize each dimension of the continuous state space\n",
        "    discretized_state = np.digitize(state, bins=bins)\n",
        "    return tuple(discretized_state)\n",
        "# Define the number of bins for discretization\n",
        "num_bins = 20\n",
        "bins = [np.linspace(-bound, bound, num_bins) for bound in env.observation_space.high]\n",
        "# Discretize the initial state\n",
        "state = env.reset()\n",
        "discretized_state = discretize_state(state, bins)"
      ],
      "metadata": {
        "id": "lxzU3pdsnclE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable\n",
        "num_actions = env.action_space.n\n",
        "# Qtable = np.zeros([num_bins[0].size] * len(env.observation_space.high) + [num_actions])\n",
        ""
      ],
      "metadata": {
        "id": "cPB3DgF8QeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, Qtable):\n",
        "    for episode in tqdm(range(n_training_episodes)):\n",
        "        # Reduce epsilon (because we need less and less exploration)\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        terminated = False\n",
        "\n",
        "        while not terminated:\n",
        "          ######################################### Step 2 #################################\n",
        "            # Choose the action (A_t) using epsilon greedy policy:\n",
        "            random_num = random.uniform(0,1)\n",
        "            # if random_num > greater than epsilon --> exploitation\n",
        "            if random_num > epsilon:\n",
        "                # Take the action with the highest value given a state\n",
        "                # np.argmax can be useful here\n",
        "                action = np.argmax(Qtable[state, :])\n",
        "            # else --> exploration\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "          ######################################### Step 3 #################################\n",
        "            # Take action A_t and observe R_t+1 and S_t+1\n",
        "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "            new_state, reward, terminated, info = env.step(action)\n",
        "\n",
        "          ######################################### Step 4 #################################\n",
        "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "            Qtable[state, action] = Qtable[state, action] + learning_rate * (reward + gamma * np.max(Qtable[new_state, :]) - Qtable[state, action])\n",
        "\n",
        "            # If terminated or truncated finish the episode\n",
        "            if terminated:\n",
        "                break\n",
        "\n",
        "            # Our next state is the new state\n",
        "            state = new_state\n",
        "    return Qtable"
      ],
      "metadata": {
        "id": "6Yv5pArjQgT-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_try = 2\n",
        "Qtable_ = np.zeros([num_try, env.observation_space.n, env.action_space.n])\n",
        "for i in range(num_try):\n",
        "  Qtable = initialize_q_table(env.observation_space.n, env.action_space.n)\n",
        "  Qtable_[i,:,:] = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, Qtable)"
      ],
      "metadata": {
        "id": "6Egf7qJFQh5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV7ySk69QkcF",
        "outputId": "cef1c6a4-3f53-4e9b-d422-05a519d2a877"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0.18261161, 0.17180984, 0.17094099, 0.16323326],\n",
              "        [0.10427457, 0.09805836, 0.09688397, 0.15244162],\n",
              "        [0.14784484, 0.13337724, 0.1298582 , 0.12420286],\n",
              "        [0.05275469, 0.05315179, 0.05212607, 0.12472221],\n",
              "        [0.20806218, 0.14988627, 0.14065381, 0.12837989],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.1577711 , 0.11167345, 0.14750589, 0.04293065],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.14664649, 0.20687632, 0.19414384, 0.26805474],\n",
              "        [0.27343613, 0.36698549, 0.28839622, 0.21142253],\n",
              "        [0.40334153, 0.35735182, 0.26878445, 0.18189807],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.30300644, 0.39768757, 0.49834309, 0.34371394],\n",
              "        [0.53056603, 0.73394277, 0.67766411, 0.5953108 ],\n",
              "        [0.        , 0.        , 0.        , 0.        ]],\n",
              "\n",
              "       [[0.18313727, 0.17172983, 0.16765094, 0.16225947],\n",
              "        [0.08327526, 0.08300035, 0.07271035, 0.15262313],\n",
              "        [0.15009553, 0.10879611, 0.11291375, 0.09862406],\n",
              "        [0.0357915 , 0.03674809, 0.03362638, 0.11092127],\n",
              "        [0.21496409, 0.16354543, 0.1363466 , 0.12361398],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.1749584 , 0.12411867, 0.13485847, 0.0389869 ],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.15050303, 0.22388906, 0.16579588, 0.2736359 ],\n",
              "        [0.24703327, 0.3740793 , 0.29073729, 0.22461095],\n",
              "        [0.38239702, 0.35408523, 0.26666899, 0.16332726],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.27524537, 0.37513409, 0.50906084, 0.36104115],\n",
              "        [0.51546465, 0.73318106, 0.65667047, 0.62709238],\n",
              "        [0.        , 0.        , 0.        , 0.        ]],\n",
              "\n",
              "       [[0.1684693 , 0.16204548, 0.16160837, 0.15854546],\n",
              "        [0.09508737, 0.09392223, 0.07914031, 0.14643618],\n",
              "        [0.14434813, 0.11194861, 0.10601982, 0.10137543],\n",
              "        [0.05833243, 0.009156  , 0.00778965, 0.01312039],\n",
              "        [0.20010738, 0.1562239 , 0.13368029, 0.11173739],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.14505159, 0.11578601, 0.16192874, 0.04683915],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.1366114 , 0.20331342, 0.19086585, 0.26019973],\n",
              "        [0.24703124, 0.36765414, 0.29772958, 0.19081554],\n",
              "        [0.38428876, 0.3231185 , 0.25449491, 0.18325797],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.        , 0.        , 0.        , 0.        ],\n",
              "        [0.2842232 , 0.37389769, 0.51990428, 0.37978973],\n",
              "        [0.49930603, 0.69125651, 0.65716275, 0.61780186],\n",
              "        [0.        , 0.        , 0.        , 0.        ]]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, n_eval_episodes, Qtable):\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    while not terminated:\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = np.argmax(Qtable[state, :])\n",
        "      new_state, reward, terminated, info = env.step(action)\n",
        "\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated:\n",
        "        break\n",
        "      state = new_state\n",
        "\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "PTxE-LdOQpd9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "for i in range(Qtable_.shape[0]):\n",
        "  mean_reward, std_reward = evaluate_agent(env, n_eval_episodes, Qtable_[i,:,:])\n",
        "  print(f\"\\nMean_reward = {mean_reward:.2f} +/- {std_reward:.2f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rSif-mFQqSl",
        "outputId": "957d61dc-4cb5-4109-e816-bdefc755a9f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1186.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean_reward = 0.71 +/- 0.45\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1101.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean_reward = 0.76 +/- 0.43\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1128.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean_reward = 0.73 +/- 0.44\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sequence of actions that has taken to reach the goal\n",
        "Qtable_selected = Qtable_[1]\n",
        "action_sequence=[]\n",
        "for j in range(Qtable_selected.shape[0]):\n",
        "    action_sequence.append(Actions[np.argmax(np.array(Qtable_selected)[j, :])])\n",
        "print(' --> '.join(action_sequence))\n",
        "env_screen = env.render(mode = 'rgb_array',)\n",
        "plt.imshow(env_screen)\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "aU77MhZ7QrtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_Sarsa(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, Qtable):\n",
        "    for episode in tqdm(range(n_training_episodes)):\n",
        "        # Reduce epsilon (because we need less and less exploration)\n",
        "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
        "        # Reset the environment\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        terminated = False\n",
        "\n",
        "        while not terminated:\n",
        "          ######################################### Step 2 #################################\n",
        "            # Choose the action (A_t) using epsilon greedy policy:\n",
        "            random_num = random.uniform(0,1)\n",
        "            # if random_num > greater than epsilon --> exploitation\n",
        "            if random_num > epsilon:\n",
        "                # Take the action with the highest value given a state\n",
        "                # np.argmax can be useful here\n",
        "                action = np.argmax(Qtable[state, :])\n",
        "            # else --> exploration\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "\n",
        "          ######################################### Step 3 #################################\n",
        "            # Take action A_t and observe R_t+1 and S_t+1\n",
        "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "            new_state, reward, terminated, info = env.step(action)\n",
        "\n",
        "          ######################################### Step 4 #################################\n",
        "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "            Qtable[state, action] = Qtable[state, action] + learning_rate * (reward + gamma * np.array(random.choices(Qtable[new_state, :],weights = [1, 1, 1, 1], k = 1)) - Qtable[state, action])\n",
        "\n",
        "            # If terminated or truncated finish the episode\n",
        "            if terminated:\n",
        "                break\n",
        "\n",
        "            # Our next state is the new state\n",
        "            state = new_state\n",
        "    return Qtable"
      ],
      "metadata": {
        "id": "8TXOlzKsQtrt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_try = 3\n",
        "Qtable_ = np.zeros([num_try, env.observation_space.n, env.action_space.n])\n",
        "for i in range(num_try):\n",
        "  Qtable = initialize_q_table(env.observation_space.n, env.action_space.n)\n",
        "  Qtable_[i,:,:] = train_Sarsa(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, Qtable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8QgiGlwQvoV",
        "outputId": "fff49f9c-1a37-4b3d-fa14-91a9d9e03577"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50000/50000 [00:51<00:00, 962.19it/s]\n",
            "100%|██████████| 50000/50000 [00:41<00:00, 1197.59it/s]\n",
            "100%|██████████| 50000/50000 [00:34<00:00, 1458.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "for i in range(Qtable_.shape[0]):\n",
        "  mean_reward, std_reward = evaluate_agent(env, n_eval_episodes, Qtable_[i,:,:])\n",
        "  print(f\"\\nMean_reward = {mean_reward:.2f} +/- {std_reward:.2f}\\n\")"
      ],
      "metadata": {
        "id": "-KDVAyiUQx1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sequence of actions that has taken to reach the goal\n",
        "Qtable_selected = Qtable_[0]\n",
        "action_sequence=[]\n",
        "for j in range(Qtable_selected.shape[0]):\n",
        "    action_sequence.append(Actions[np.argmax(np.array(Qtable_selected)[j, :])])\n",
        "print(' --> '.join(action_sequence))\n",
        "env_screen = env.render(mode = 'rgb_array',)\n",
        "plt.imshow(env_screen)\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "fvG6sledQzTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vCNa8psMRyw8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}