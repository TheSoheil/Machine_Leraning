{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This Program is developed by Soheil Hekmat For Q3 part a and b"
      ],
      "metadata": {
        "id": "7QSMfRxdCdh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "4DDgFR8KCsm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gym import Env\n",
        "from gym.spaces import Discrete\n",
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "9HXc5LQ3RAn8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making The Environment Class"
      ],
      "metadata": {
        "id": "bBVOKw0eCwoD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoheilEnv(Env):\n",
        "    def __init__(self):\n",
        "        # Actions we can take, we can have 4 actions in diameter\n",
        "        self.action_space = Discrete(4)\n",
        "        # all states array, not that we have states 1 to 99 but in array it's 0 to 98 so we should use +1 in every use\n",
        "        self.observation_space = Discrete(99)\n",
        "        # Set start states s1 and s2\n",
        "        st = [81,85]\n",
        "        self.state = random.choice(st)\n",
        "        # Set maximum step\n",
        "        self.max_step = 200\n",
        "        self.grid = [['.' for _ in range(11)] for _ in range(9)]\n",
        "\n",
        "    def step(self, action):\n",
        "        # Apply action\n",
        "        ac =[-12, -10, 10, 12]\n",
        "        # 0 == -12\n",
        "        # 1 == -10\n",
        "        # 2 == 10\n",
        "        # 3 == 12\n",
        "        self.state += ac[action]\n",
        "        # Reduce maximum step by 1 time\n",
        "        self.max_step -= 1\n",
        "\n",
        "        # Set a default value for reward in case it's not set in your environment\n",
        "        reward = 0\n",
        "        # Calculate reward\n",
        "        white = [15,16,17,18,19,20,21,24,25,26,27,28,31,32,35,36,37,42,43,47,48,49,50,51,52,53,54,57,58,62,63,68,69,70,71,72,73,74,75,76,79,80,81,82,84,85,86,87]\n",
        "        gray = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,22,23,30,33,34,38,39,40,41,44,45,46,55,56,59,60,61,64,65,66,67,77,78,83,88,89,90,91,92,93,94,95,96,97,98,99]\n",
        "        green = 29\n",
        "        if self.state in white:\n",
        "            reward =-1\n",
        "        if self.state in gray:\n",
        "            reward = -5\n",
        "        if self.state == green:\n",
        "            reward = 8\n",
        "\n",
        "        # Check if path finding is done\n",
        "        if self.state in gray or self.state == green:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        # Set placeholder for info\n",
        "        info = {}\n",
        "\n",
        "        # Return step information\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def render(self):\n",
        "        for row in range(9):\n",
        "          for col in range(11):\n",
        "            if row == 0 or row == 8 or col == 0 or col ==10:\n",
        "              self.grid[row][col] = 'X'\n",
        "            if (row,col) == (1,1) or (row,col) == (1,2) or (row,col) == (2,7) or (row,col) == (3,4) or (row,col) == (3,5) or (row,col) == (3,6) or (row,col) == (3,7):\n",
        "              self.grid[row][col] = 'X'\n",
        "            if (row,col) == (4,1) or (row,col) == (5,3) or (row,col) == (5,4) or (row,col) == (5,5) or (row,col) == (5,8) or (row,col) == (5,9) or (row,col) == (7,5):\n",
        "              self.grid[row][col] = 'X'\n",
        "        for i in range(9):\n",
        "          for j in range(11):\n",
        "            if (i,j) == (3-1,7-1):\n",
        "              print('G', end=' ')\n",
        "            if (i,j) == (8-1,4-1):\n",
        "              print('S', end=' ')\n",
        "            if (i,j) == (8-1,8-1):\n",
        "              print('S', end=' ')\n",
        "            if (i,j) != (8-1,8-1) and (i,j) != (8-1,4-1) and (i,j) != (3-1,7-1):\n",
        "              print(self.grid[i][j], end=' ')\n",
        "          print()\n",
        "        print()\n",
        "    def reset(self):\n",
        "        # Reset robot's position to s1 or s2\n",
        "        st = [81,85]\n",
        "        self.state = random.choice(st)\n",
        "        # Reset max step\n",
        "        self.max_step = 200\n",
        "        # Initialize the grid with special characters\n",
        "        # for i in range(9):\n",
        "        #     for j in range(11):\n",
        "        #         for cell, symbol in self.special_cells:\n",
        "        #             if (i, j) == cell:\n",
        "        #                 self.grid[i][j] = symbol\n",
        "\n",
        "        # Return state adjusted to the range 1 to 99 (descrete(99) is 0 to 98)\n",
        "        return self.state\n",
        ""
      ],
      "metadata": {
        "id": "5AIVkY0aRWDB"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment calling"
      ],
      "metadata": {
        "id": "JTSUubodC74C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = SoheilEnv()"
      ],
      "metadata": {
        "id": "atB8ZNqzY3Ne"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.action_space.sample()\n",
        "#env.observation_space.sample() + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjwHKYUNcGR1",
        "outputId": "8d38692f-e06c-4999-a858-7597c3268fca"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing some actions and rewards"
      ],
      "metadata": {
        "id": "DsiOfJ8oC_fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 10\n",
        "for episode in range(1, episodes+1):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        n_state, reward, done, info = env.step(action)\n",
        "        score+=reward\n",
        "    print('Episode:{} Score:{} FinalState:{}'.format(episode, score, n_state))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BShaq9w4cPQG",
        "outputId": "133f8799-5f5f-40f5-d6f4-e8814a935c28"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:1 Score:-7 FinalState:93\n",
            "Episode:2 Score:-8 FinalState:59\n",
            "Episode:3 Score:-5 FinalState:95\n",
            "Episode:4 Score:-7 FinalState:93\n",
            "Episode:5 Score:-6 FinalState:65\n",
            "Episode:6 Score:-5 FinalState:95\n",
            "Episode:7 Score:-5 FinalState:95\n",
            "Episode:8 Score:-5 FinalState:91\n",
            "Episode:9 Score:-10 FinalState:65\n",
            "Episode:10 Score:-5 FinalState:91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization"
      ],
      "metadata": {
        "id": "Jd_DveOaDDha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I592DhOioV3",
        "outputId": "bc457c72-46cd-42c8-be1f-d324b3c4217d"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X X X X X X X X X X X \n",
            "X X X . . . . . . . X \n",
            "X . . . . . G X . . X \n",
            "X . . . X X X X . . X \n",
            "X X . . . . . . . . X \n",
            "X . . X X X . . X X X \n",
            "X . . . . . . . . . X \n",
            "X . . S . X . S . . X \n",
            "X X X X X X X X X X X \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.observation_space.n"
      ],
      "metadata": {
        "id": "HXQIgG5GNdzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining env and num actions and states\n",
        "num_states = env.observation_space.n\n",
        "num_actions = env.action_space.n\n",
        "# Initialize the transition probability matrix P\n",
        "P = np.zeros((num_states, num_actions, num_states))\n",
        "\n",
        "# Fill in the transition probability matrix P based on your environment's dynamics\n",
        "\n",
        "# Initialize the state value function V, state-action value function Q, and policy\n",
        "V = np.zeros(num_states)\n",
        "Q = np.zeros((num_states, num_actions))\n",
        "policy = np.zeros(num_states+1, dtype=int)\n",
        "\n",
        "# Policy Iteration\n",
        "def policy_iteration():\n",
        "    max_steps = 1000\n",
        "    gamma = 1\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        state = env.reset()\n",
        "        step = 0\n",
        "        terminated = False\n",
        "        # Policy Evaluation\n",
        "        # for s in range(num_states):\n",
        "        s = 0\n",
        "        while not terminated:\n",
        "            action = policy[s]\n",
        "            next_state, reward, terminated, _ = env.step(action)\n",
        "            P = np.zeros((num_states, num_actions, num_states))\n",
        "            for s in range(num_states):\n",
        "              for a in range(num_actions):\n",
        "                if a == 0:  # action -12\n",
        "                    P[s, a, max(s - 12, 0)] = 1\n",
        "                elif a == 1:  # action -10\n",
        "                    P[s, a, max(s - 10, 0)] = 1\n",
        "                elif a == 2:  # action +10\n",
        "                    P[s, a, min(s + 10, 98)] = 1\n",
        "                elif a == 3:  # action +12\n",
        "                    P[s, a, min(s + 12, 98)] = 1\n",
        "            V[s] = reward + gamma * np.sum(P[s, action, :] * V)\n",
        "            Q[s, action] = reward + gamma * np.sum(P[s, action, :] * V)\n",
        "            if terminated:\n",
        "                break\n",
        "            print(Q[s, action])\n",
        "            s +=1\n",
        "        # Policy Improvement\n",
        "        policy_stable = True\n",
        "        for s in range(num_states):\n",
        "            old_action = policy[s]\n",
        "            policy[s] = np.argmax(Q[s, :])\n",
        "            if old_action != policy[s]:\n",
        "                policy_stable = False\n",
        "\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return policy, Q\n",
        "\n",
        "# Value Iteration\n",
        "def value_iteration():\n",
        "    max_steps = 1000\n",
        "    gamma = 1\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        for s in range(num_states):\n",
        "            action_values = np.sum(P[s, :, :] * (1 + gamma * V), axis=1)\n",
        "            policy[s] = np.argmax(action_values)\n",
        "            Q[s, :] = action_values\n",
        "            V[s] = np.max(action_values)\n",
        "\n",
        "    return policy, Q\n",
        "\n",
        "# Run Policy Iteration\n",
        "policy_pi, Q_pi = policy_iteration()\n",
        "\n",
        "# Run Value Iteration\n",
        "policy_vi, Q_vi = value_iteration()\n",
        "\n",
        "# Print results\n",
        "print(\"Policy Iteration - Optimal Policy:\")\n",
        "print(policy_pi)\n",
        "print(\"Policy Iteration - Optimal State-Action Values:\")\n",
        "print(Q_pi)\n",
        "\n",
        "print(\"\\nValue Iteration - Optimal Policy:\")\n",
        "print(policy_vi)\n",
        "print(\"Value Iteration - Optimal State-Action Values:\")\n",
        "print(Q_vi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sc08LdwPjprW",
        "outputId": "c90927a9-d637-4087-a0f1-37ace28add0c"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1.0\n",
            "-1.0\n",
            "-1.0\n",
            "Policy Iteration - Optimal Policy:\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Policy Iteration - Optimal State-Action Values:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n",
            "\n",
            "Value Iteration - Optimal Policy:\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Value Iteration - Optimal State-Action Values:\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path"
      ],
      "metadata": {
        "id": "J-1DMMAngsIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "policy iteration"
      ],
      "metadata": {
        "id": "IipH2B9zgvuY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sequence of actions that has taken to reach the goal\n",
        "Actions =  {0: 'Up Right',\n",
        "            1: 'Up Forward',\n",
        "            2: 'Down Left',\n",
        "            3: 'Down Right'}\n",
        "Qtable_selected = Q_pi\n",
        "action_sequence=[]\n",
        "for j in range(Qtable_selected.shape[0]):\n",
        "    action_sequence.append(Actions[np.argmax(np.array(Qtable_selected)[j, :])])\n",
        "print(' --> '.join(action_sequence))"
      ],
      "metadata": {
        "id": "VClL2Fz93CRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "value iteration"
      ],
      "metadata": {
        "id": "NobHAEQwgyXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_selected = Q_vi\n",
        "action_sequence=[]\n",
        "for j in range(Qtable_selected.shape[0]):\n",
        "    action_sequence.append(Actions[np.argmax(np.array(Qtable_selected)[j, :])])\n",
        "print(' --> '.join(action_sequence))"
      ],
      "metadata": {
        "id": "wyms_OESgmZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}